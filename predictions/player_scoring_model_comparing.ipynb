{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Machine Learning Comparision\n",
    "\n",
    "A tool designed to find the factors which relate to a player scoring a try."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"..\")\n",
    "import ENVIRONMENT_VARIABLES as EV \n",
    "\n",
    "import json\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, roc_auc_score\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "from xgboost import XGBClassifier\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "!python -m pip install imblearn\n",
    "!python -m pip install xgboost\n",
    "!python -m pip install catboost"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Select Round and Team"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ðŸ”§ CHANGE THESE VARIABLES AS NEEDED\n",
    "selected_team = 'Knights'  \n",
    "prediction_round = 22\n",
    "YEAR = 2024\n",
    "ROUNDS = 31\n",
    "SELECTION = 'NRL'\n",
    "\n",
    "TRAIN_MODEL = True # Deselect this once it has been trained, so you can change the variables above"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing the Player Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "variables =[\"Year\", \"Win\", \"Defense\", \"Attack\", \"Margin\", \"Home\", \"Versus\",  \"Round\"]\n",
    "\n",
    "player_variables =[\"Name\", \"Position\", \"Points\", \"Tries\", \"All Run Metres\", \"Tackle Breaks\", \"Tackle Efficiency\", \"Kicking Metres\",  \"Offloads\", 'All Runs', 'Line Breaks', 'Post Contact Metres','Dummy Half Runs', 'Passes','Receipts', 'Errors', 'Sin Bins', \"Versus\", ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TEAMS = EV.TEAMS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if TRAIN_MODEL:\n",
    "    years_arr = {}\n",
    "    with open(f'../data/{SELECTION}/{YEAR}/{SELECTION}_player_statistics_{YEAR}.json', 'r') as file:\n",
    "        data = json.load(file)\n",
    "        data = data['PlayerStats']\n",
    "        years_arr[YEAR] = data[0][str(YEAR)]\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create the structured dataframe\n",
    "df = pd.DataFrame(columns=[f\"{team} {variable}\" for team in TEAMS for variable in player_variables])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if TRAIN_MODEL:\n",
    "    p_dfs = {}\n",
    "\n",
    "    def custom_sort(column_name):\n",
    "        year, num = column_name.split('-')\n",
    "        return int(year), int(num)\n",
    "\n",
    "\n",
    "    for team in TEAMS:\n",
    "        p_dfs[team] = pd.DataFrame(columns=[f\"{year}-{round+1}\" for round in range(0, ROUNDS) for year in [YEAR]])\n",
    "        p_dfs[team] = p_dfs[team][sorted(p_dfs[team].columns, key=custom_sort)]\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if TRAIN_MODEL:\n",
    "    team_players = {}\n",
    "    ladder = {}  # Tracks ladder info per team\n",
    "\n",
    "    for i in range(0, ROUNDS):\n",
    "        try:\n",
    "            round_data = years_arr[YEAR][i]\n",
    "            round_data = round_data[str(i)]\n",
    "            round_ladder_snapshot = {}\n",
    "\n",
    "            for round_game in round_data:\n",
    "                for game in round_game:\n",
    "                    game_split = game.split(\"-\")\n",
    "                    game_year = game_split[0]\n",
    "                    game_round = game_split[1]\n",
    "\n",
    "                    game_split = game.split(\"v\")\n",
    "                    home_team = \" \".join(game_split[0].split(\"-\")[2:]).replace(\"-\", \" \").strip()\n",
    "                    away_team = \" \".join(game_split[-1:]).replace(\"-\", \" \").strip()\n",
    "\n",
    "                    players = round_game[game]\n",
    "\n",
    "                    # Remove duplicates\n",
    "                    seen = set()\n",
    "                    unique_dicts = []\n",
    "                    for d in players:\n",
    "                        items = tuple(sorted(d.items()))\n",
    "                        if items not in seen:\n",
    "                            seen.add(items)\n",
    "                            unique_dicts.append(d)\n",
    "\n",
    "                    players = unique_dicts\n",
    "\n",
    "                    # Calculate points\n",
    "                    home_team_points = sum([(int(x['Points'].replace('-', '0'))) for x in players[:18]])\n",
    "                    away_team_points = sum([(int(x['Points'].replace('-', '0'))) for x in players[18:]])\n",
    "\n",
    "                    # Update ladder\n",
    "                    for team, points_for, points_against in [\n",
    "                        (home_team, home_team_points, away_team_points),\n",
    "                        (away_team, away_team_points, home_team_points)\n",
    "                    ]:\n",
    "                        if team not in ladder:\n",
    "                            ladder[team] = {\n",
    "                                'points': 0,\n",
    "                                'for': 0,\n",
    "                                'against': 0,\n",
    "                                'games': 0\n",
    "                            }\n",
    "\n",
    "                        ladder[team]['for'] += points_for\n",
    "                        ladder[team]['against'] += points_against\n",
    "                        ladder[team]['games'] += 1\n",
    "\n",
    "                    # Assign match result points (Win = 2, Draw = 1)\n",
    "                    if home_team_points > away_team_points:\n",
    "                        ladder[home_team]['points'] += 2\n",
    "                    elif away_team_points > home_team_points:\n",
    "                        ladder[away_team]['points'] += 2\n",
    "                    else:\n",
    "                        ladder[home_team]['points'] += 1\n",
    "                        ladder[away_team]['points'] += 1\n",
    "\n",
    "                    # Snapshot the ladder using proper tiebreaker: points, then 'for' total\n",
    "                    ladder_snapshot = sorted(\n",
    "                        ladder.items(),\n",
    "                        key=lambda x: (x[1]['points'], x[1]['for']),\n",
    "                        reverse=True\n",
    "                    )\n",
    "\n",
    "                    for pos, (team, _) in enumerate(ladder_snapshot, start=1):\n",
    "                        round_ladder_snapshot[team] = pos  # always set a valid int pos\n",
    "\n",
    "                    # ðŸ–¨ï¸ Print AFTER ladder snapshot\n",
    "                    print(\n",
    "                        f\"> {home_team} (Ladder Pos: {round_ladder_snapshot[home_team]}): {home_team_points} \"\n",
    "                        f\"v {away_team} (Ladder Pos: {round_ladder_snapshot[away_team]}): {away_team_points} \"\n",
    "                        f\"- {game_year} - Round {game_round}\"\n",
    "                    )\n",
    "\n",
    "                    # Build player stats\n",
    "                    player_round_stats = {}\n",
    "\n",
    "                    for idx, player in enumerate(players):\n",
    "                        versus = home_team if idx >= 18 else away_team\n",
    "                        team = away_team if idx >= 18 else home_team\n",
    "\n",
    "                        vals = [player[val] for val in player_variables[:-1]]\n",
    "                        vals.append(TEAMS.index(versus))  # versus team index\n",
    "                        vals.append(round_ladder_snapshot[team])      # team ladder pos\n",
    "                        vals.append(round_ladder_snapshot[versus])    # opponent ladder pos\n",
    "\n",
    "                        player_round_stats[vals[0]] = vals[1:]\n",
    "\n",
    "                    player_round_stats = list(player_round_stats.items())\n",
    "                    player_round_stats_home, player_round_stats_away = player_round_stats[:18], player_round_stats[18:]\n",
    "                    p_dfs[home_team][f\"{YEAR}-{i+1}\"] = player_round_stats_home\n",
    "                    p_dfs[away_team][f\"{YEAR}-{i+1}\"] = player_round_stats_away\n",
    "\n",
    "        except Exception as ex:\n",
    "            print(f\"Error in round {i}: {ex}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare and Clean Data from All Teams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if TRAIN_MODEL:\n",
    "    all_long_dfs = []\n",
    "\n",
    "    for team, df in p_dfs.items():\n",
    "        long_df = df.melt(ignore_index=False, var_name=\"Round\", value_name=\"PlayerStats\").dropna()\n",
    "        long_df[['Name', 'Stats']] = pd.DataFrame(long_df['PlayerStats'].tolist(), index=long_df.index)\n",
    "\n",
    "        stat_cols = [\"Position\", \"Points\", \"Tries\", \"All Run Metres\", \"Tackle Breaks\", \"Tackle Efficiency\", \"Kicking Metres\", \"Offloads\", 'All Runs', 'Line Breaks', 'Post Contact Metres','Dummy Half Runs', 'Passes','Receipts', 'Errors', 'Sin Bins','Versus', 'Home Pos', 'Away Pos']\n",
    "        long_df[stat_cols] = pd.DataFrame(long_df['Stats'].tolist(), index=long_df.index)\n",
    "\n",
    "        def clean_stat(val):\n",
    "            if isinstance(val, str):\n",
    "                if val == '-' or val.strip() == '':\n",
    "                    return np.nan\n",
    "                if '%' in val:\n",
    "                    return float(val.replace('%', ''))\n",
    "            try:\n",
    "                return float(val)\n",
    "            except:\n",
    "                return np.nan\n",
    "\n",
    "        for col in stat_cols:\n",
    "            if col != 'Position':\n",
    "                long_df[col] = long_df[col].apply(clean_stat)\n",
    "\n",
    "        long_df['Round'] = long_df['Round'].str.extract(r'(\\d+)$').astype(int)\n",
    "        long_df['Team'] = team\n",
    "        all_long_dfs.append(long_df)\n",
    "\n",
    "    # Combine all team DataFrames\n",
    "    long_df_all = pd.concat(all_long_dfs).reset_index(drop=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "long_df_all[long_df_all['Name'] == 'Kalyn Ponga'].head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fill in Missing Values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if TRAIN_MODEL:\n",
    "    # Fill missing values\n",
    "    fill_cols = ['Tries', 'Points', 'Kicking Metres', 'Offloads', 'All Runs', 'Line Breaks', 'Post Contact Metres','Dummy Half Runs', 'Passes','Receipts', 'Errors', 'Sin Bins',]\n",
    "    for col in fill_cols:\n",
    "        long_df_all[col] = long_df_all[col].fillna(0)\n",
    "\n",
    "    # Target for training: DidScoreTry\n",
    "    long_df_all['DidScoreTry'] = long_df_all['Tries'].apply(lambda x: 1 if x > 0 else 0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Encode player positions as integers\n",
    "position_encoder = LabelEncoder()\n",
    "long_df_all['Position'] = position_encoder.fit_transform(long_df_all['Position'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare Training Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if TRAIN_MODEL:\n",
    "\n",
    "    features = [ 'Home Pos', 'Away Pos', \"All Run Metres\",\"Tackle Breaks\", \"Position\", \"Kicking Metres\", \"Offloads\", 'All Runs', 'Line Breaks', 'Post Contact Metres','Dummy Half Runs', 'Passes','Receipts', 'Errors']\n",
    "\n",
    "    # Prepare training data: all rounds before the prediction round\n",
    "    train_df = long_df_all[long_df_all['Round'] < prediction_round].copy()\n",
    "    train_df['DidScoreTry'] = train_df['Tries'].apply(lambda x: 1 if x > 0 else 0)\n",
    "\n",
    "    # Features and labels\n",
    "    train_data = train_df[features + ['DidScoreTry']].dropna()\n",
    "    X = train_data[features]\n",
    "    y = train_data['DidScoreTry']\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from catboost import CatBoostClassifier\n",
    "\n",
    "if TRAIN_MODEL:\n",
    "    X_train, X_val, y_train, y_val = train_test_split(X, y, stratify=y, random_state=42)\n",
    "\n",
    "    model = CatBoostClassifier(\n",
    "        iterations=1000,\n",
    "        learning_rate=0.01,  # Slower learning = better generalization\n",
    "        depth=8,  # More complex interactions\n",
    "        l2_leaf_reg=3,\n",
    "        loss_function='Logloss',\n",
    "        eval_metric='AUC',\n",
    "        class_weights=[1, 4],  # Slightly lower weight for balance\n",
    "        random_seed=42,\n",
    "        verbose=100,\n",
    "        early_stopping_rounds=100\n",
    "    )\n",
    "\n",
    "\n",
    "    model.fit(X_train, y_train, eval_set=(X_val, y_val), early_stopping_rounds=50)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import (\n",
    "    accuracy_score, roc_auc_score, roc_curve,\n",
    "    confusion_matrix, ConfusionMatrixDisplay,\n",
    "    precision_recall_curve, PrecisionRecallDisplay\n",
    ")\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Predict on validation set\n",
    "y_val_pred = model.predict(X_val)\n",
    "y_val_prob = model.predict_proba(X_val)[:, 1]\n",
    "\n",
    "# Core metrics\n",
    "val_accuracy = accuracy_score(y_val, y_val_pred)\n",
    "val_auc = roc_auc_score(y_val, y_val_prob)\n",
    "\n",
    "print(f\"ðŸ” Model Evaluation on Validation Set (All Teams)\")\n",
    "print(f\"Accuracy: {val_accuracy:.2f}\")\n",
    "print(f\"AUC: {val_auc:.2f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import f1_score, precision_recall_curve\n",
    "\n",
    "# Get precision, recall, thresholds\n",
    "precision, recall, thresholds = precision_recall_curve(y_val, y_val_prob)\n",
    "f1 = 2 * (precision * recall) / (precision + recall + 1e-8)\n",
    "\n",
    "# Find best threshold by max F1\n",
    "best_idx = f1.argmax()\n",
    "best_threshold = thresholds[best_idx]\n",
    "\n",
    "print(f\"Best Threshold: {best_threshold:.2f}\")\n",
    "print(f\"F1 Score at Best Threshold: {f1[best_idx]:.2f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ðŸ” Overall evaluation for the prediction round\n",
    "all_teams_round_df = long_df_all[long_df_all['Round'] == prediction_round].copy()\n",
    "all_teams_round_df['Tries'] = all_teams_round_df['Tries'].fillna(0)\n",
    "all_teams_round_df = all_teams_round_df.dropna(subset=features)\n",
    "\n",
    "if not all_teams_round_df.empty:\n",
    "    X_test_all = all_teams_round_df[features]\n",
    "    all_teams_round_df['Try_Prob'] = model.predict_proba(X_test_all)[:, 1]\n",
    "    all_teams_round_df['DidScoreTry'] = all_teams_round_df['Tries'].apply(lambda x: 1 if x > 0 else 0)\n",
    "    y_pred_all = model.predict(X_test_all)\n",
    "\n",
    "    try:\n",
    "        accuracy_all = accuracy_score(all_teams_round_df['DidScoreTry'], y_pred_all)\n",
    "        auc_all = roc_auc_score(all_teams_round_df['DidScoreTry'], all_teams_round_df['Try_Prob'])\n",
    "        print(f\"\\nðŸ”Ž Overall - Round {prediction_round} Accuracy: {accuracy_all:.2f}, AUC: {auc_all:.2f}\")\n",
    "    except ValueError:\n",
    "        print(f\"\\nâš ï¸ Overall - Round {prediction_round}: Not enough class variety to calculate AUC\")\n",
    "else:\n",
    "    print(f\"\\nâŒ Overall - Round {prediction_round}: No valid data\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"\\nðŸ“Š Per-Team Evaluation:\")\n",
    "\n",
    "for selected_team_ in TEAMS:\n",
    "    # Predict for selected team in the selected round\n",
    "    team_round_df = long_df_all[\n",
    "        (long_df_all['Round'] == prediction_round) &\n",
    "        (long_df_all['Team'] == selected_team_)\n",
    "    ].copy()\n",
    "\n",
    "    # Clean and prepare\n",
    "    team_round_df['Tries'] = team_round_df['Tries'].fillna(0)\n",
    "    team_round_df = team_round_df.dropna(subset=features)\n",
    "\n",
    "    if team_round_df.empty:\n",
    "        print(f\"âŒ {selected_team_} - Round {prediction_round}: No data (possibly a bye or all rows invalid)\")\n",
    "        continue\n",
    "\n",
    "    X_test = team_round_df[features]\n",
    "    team_round_df['Try_Prob'] = model.predict_proba(X_test)[:, 1]\n",
    "    team_round_df['DidScoreTry'] = team_round_df['Tries'].apply(lambda x: 1 if x > 0 else 0)\n",
    "\n",
    "    y_pred = model.predict(X_test)\n",
    "\n",
    "    try: \n",
    "        accuracy = accuracy_score(team_round_df['DidScoreTry'], y_pred)\n",
    "        auc = roc_auc_score(team_round_df['DidScoreTry'], team_round_df['Try_Prob'])\n",
    "        print(f\"ðŸŸ¢ {selected_team_} - Round {prediction_round} Accuracy: {accuracy:.2f}, AUC: {auc:.2f}\")\n",
    "    except ValueError:\n",
    "        print(f\"âš ï¸ {selected_team_} - Round {prediction_round}: Not enough class variety to calculate AUC\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fpr, tpr, _ = roc_curve(y_val, y_val_prob)\n",
    "\n",
    "plt.figure(figsize=(6, 5))\n",
    "plt.plot(fpr, tpr, label=f\"AUC = {val_auc:.2f}\")\n",
    "plt.plot([0, 1], [0, 1], 'k--', alpha=0.6)\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('ROC Curve')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "precision, recall, _ = precision_recall_curve(y_val, y_val_prob)\n",
    "pr_disp = PrecisionRecallDisplay(precision=precision, recall=recall)\n",
    "pr_disp.plot()\n",
    "plt.title('Precision-Recall Curve')\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "feature_importances = model.get_feature_importance()\n",
    "feature_names = X.columns\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.barh(feature_names, feature_importances)\n",
    "plt.title(\"Feature Importance\")\n",
    "plt.gca().invert_yaxis()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict for selected team in the selected round\n",
    "team_round_df = long_df_all[\n",
    "    (long_df_all['Round'] == prediction_round) &\n",
    "    (long_df_all['Team'] == selected_team)\n",
    "].copy()\n",
    "\n",
    "# Clean and prepare\n",
    "team_round_df['Tries'] = team_round_df['Tries'].fillna(0)\n",
    "team_round_df = team_round_df.dropna(subset=features)\n",
    "\n",
    "X_test = team_round_df[features]\n",
    "team_round_df['Try_Prob'] = model.predict_proba(X_test)[:, 1]\n",
    "team_round_df['DidScoreTry'] = team_round_df['Tries'].apply(lambda x: 1 if x > 0 else 0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "team_round_df.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# View sorted predictions for the selected team and round\n",
    "team_round_df[['Name', 'Try_Prob', 'DidScoreTry']].sort_values(by='Try_Prob', ascending=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "team_round_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Evaluate if needed\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "try: \n",
    "    accuracy = accuracy_score(team_round_df['DidScoreTry'], y_pred)\n",
    "    auc = roc_auc_score(team_round_df['DidScoreTry'], team_round_df['Try_Prob'])\n",
    "except ValueError:\n",
    "    print('The team you are trying to predict for likely had a bye')\n",
    "\n",
    "\n",
    "\n",
    "print(f\"{selected_team} - Round {prediction_round} Accuracy: {accuracy:.2f}\")\n",
    "print(f\"{selected_team} - Round {prediction_round} AUC: {auc:.2f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
